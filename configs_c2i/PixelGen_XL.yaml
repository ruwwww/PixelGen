# lightning.pytorch==2.4.0
seed_everything: 1234
tags:
  exp: &exp PixelGen_XL
huggingface_cache_dir: null
trainer:
  default_root_dir: ./universal_pix_workdirs
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: bf16-mixed
  logger:
      class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        project: universal_flow_JiT_XL_bs256
        name: *exp
  num_sanity_val_steps: 0
  max_steps: 800000 # 800k for 160 epochs, 4000k for 800 epochs
  val_check_interval: 100000
  check_val_every_n_epoch: null
  log_every_n_steps: 50
  deterministic: null
  inference_mode: true
  use_distributed_sampler: false
  callbacks:
    - class_path: src.callbacks.model_checkpoint.CheckpointHook
      init_args:
        every_n_train_steps: 100000
        save_top_k: -1
        save_last: true
    - class_path: src.callbacks.save_images.SaveImagesHook
      init_args:
         save_dir: val_ode50_cfg2.25
         save_compressed: true
    # - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    #   init_args:
    #     logging_interval: 'step'
    #     log_momentum: False  
  plugins:
    - src.plugins.bd_env.BDEnvironment
model:
  vae:
    class_path: src.models.autoencoder.pixel.PixelAE
    init_args:
       scale: 1.0
  denoiser:
    class_path: src.models.transformer.JiT.JiT
    init_args:
      input_size: 256
      patch_size: 16
      in_channels: 3
      hidden_size: &hidden_dim 1152
      depth: 28
      num_heads: 16
      mlp_ratio: 4.0
      attn_drop: 0.0
      proj_drop: 0.1 # please enable proj_drop when training a DiT-XL with lots of parameters
      num_classes: &num_classes 1000
      use_bottleneck: false
      in_context_len: 32
      in_context_start: 8
  conditioner:
    class_path: src.models.conditioner.class_label.LabelConditioner
    init_args:
      num_classes: *num_classes
  diffusion_trainer:
    class_path: src.diffusion.flow_matching.training_repa_JiT_LPIPS_DINO_NoiseGating.REPATrainer
    init_args:
      lognorm_t: true
      encoder:
        class_path: src.models.encoder.DINOv2
        init_args:
          weight_path: ~/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth
      align_layer: 8
      proj_denoiser_dim: *hidden_dim
      proj_hidden_dim: *hidden_dim
      proj_encoder_dim: 768
      null_condition_p: 0.1
      P_mean: -0.8
      P_std: 0.8
      t_eps: 0.05
      scheduler: &scheduler src.diffusion.flow_matching.scheduling.LinearScheduler
      lpips_weight: 0.1
      dino_weight: 0.01
      percept_t_threshold: 0.3
  diffusion_sampler:
    class_path: src.diffusion.flow_matching.sampling.HeunSamplerJiT
    init_args:
      exact_henu: true
      num_steps: 50
      guidance: 2.25 # Guidance scale is set to 2.25 for 160 epochs. Please set to 1.0 for the experiments without CFG.
      timeshift: 2.0 # For XL version, the timesift is set to 2.0 during all inference.
      guidance_interval_min: 0.1
      guidance_interval_max: 0.9
      scheduler: *scheduler
      w_scheduler: src.diffusion.flow_matching.scheduling.LinearScheduler
      guidance_fn: src.diffusion.base.guidance.simple_guidance_fn
      step_fn: src.diffusion.flow_matching.sampling.ode_step_fn
  ema_tracker:
    class_path: src.callbacks.simple_ema.SimpleEMA
    init_args:
      decay: 0.9999
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      weight_decay: 0.0
  # lr_scheduler:
  #   class_path: src.utils.lr_scheduler.ConstantWithWarmup
  #   init_args:
  #     num_warmup_steps: 5005
data:
  train_dataset:
    class_path: src.data.dataset.imagenet.PixImageNet
    init_args:
      root: /data/datasets/ImageNet-1K/train # set your data dir
      resolution: 256
  eval_dataset:
    class_path: src.data.dataset.randn.ClassLabelRandomNDataset
    init_args:
      num_classes: 1000
      max_num_instances: 50000
      latent_shape:
        - 3
        - 256
        - 256
  pred_dataset:
    class_path: src.data.dataset.randn.ClassLabelRandomNDataset
    init_args:
      num_classes: *num_classes
      max_num_instances: 50000
      noise_scale: 1.0
      latent_shape:
        - 3
        - 256
        - 256
  train_batch_size: 32
  train_num_workers: 4
  pred_batch_size: 32
  pred_num_workers: 1
