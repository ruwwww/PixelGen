# lightning.pytorch==2.4.0
seed_everything: true
tags:
  exp: &exp pretraining_pix512
torch_hub_dir: ~/.cache/torch/hub
ckpt_path: /data/github_project/DeCo/universal_pix_t2i_workdirs/exp_pretraining_pix256/epoch=0-step=200000.ckpt
huggingface_cache_dir: null
trainer:
  default_root_dir: ./universal_pix_t2i_workdirs
  accelerator: auto
  strategy: ddp
  devices: auto
  num_nodes: 1
  accumulate_grad_batches: 4
  precision: bf16-mixed
  logger:
      class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        project: universal_flow_JiT_t2i
        name: *exp
  num_sanity_val_steps: 2
  max_steps: 300000
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  val_check_interval: 40000
  check_val_every_n_epoch: null
  log_every_n_steps: 50
  deterministic: null
  inference_mode: true
  use_distributed_sampler: false
  callbacks:
    - class_path: src.callbacks.model_checkpoint.CheckpointHook
      init_args:
        every_n_train_steps: 10000
        save_top_k: -1
        save_last: true
    - class_path: src.callbacks.save_images.SaveImagesHook
      init_args:
         save_dir: val
  plugins:
    - src.plugins.bd_env.BDEnvironment
model:
  vae:
    class_path: src.models.autoencoder.pixel.PixelAE
  denoiser:
    class_path: src.models.transformer.JiT_T2I.JiT_T2I
    init_args:
      patch_size: 16
      input_size: 512
      in_channels: 3
      hidden_size: &hidden_dim 1536
      num_blocks: 16
      num_groups: 24
      txt_embed_dim: &txt_embed_dim 2048
      txt_max_length: 128
      bottleneck_dim: 256
      num_text_blocks: 4
  conditioner:
    class_path: src.models.conditioner.qwen3_text_encoder.Qwen3TextEncoder
    init_args:
      weight_path: /data/pretrained_weights/Qwen/Qwen3-1.7B
      embed_dim: *txt_embed_dim
      max_length: 128
  diffusion_trainer:
    class_path: src.diffusion.flow_matching.training_repa_JiT_LPIPS_DINO_clamp.REPATrainer
    init_args:
      lognorm_t: true
      encoder:
        class_path: src.models.encoder.DINOv2
        init_args:
          weight_path: ~/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth
      align_layer: 8
      proj_denoiser_dim: *hidden_dim
      proj_hidden_dim: *hidden_dim
      proj_encoder_dim: 768
      P_mean: -0.8
      P_std: 0.8
      t_eps: 0.05
      scheduler: &scheduler src.diffusion.flow_matching.scheduling.LinearScheduler
      lpips_weight: 0.1
      dino_weight: 0.01
      percept_t_threshold: 0.3
  diffusion_sampler:
    class_path: src.diffusion.flow_matching.sampling.EulerSamplerJiT
    init_args:
      num_steps: 100
      guidance: 4.0
      scheduler: *scheduler
      w_scheduler: src.diffusion.flow_matching.scheduling.LinearScheduler
      guidance_fn: src.diffusion.base.guidance.simple_guidance_fn
      step_fn: src.diffusion.flow_matching.sampling.ode_step_fn
  ema_tracker:
    class_path: src.callbacks.simple_ema.SimpleEMA
    init_args:
      decay: 0.9999
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      betas:
        - 0.9
        - 0.95
      weight_decay: 0.00
data:
  train_dataset:
    class_path: src.data.dataset.blip3o_dataset.WebDatasetPackedDataset
    init_args:
      urls: [/data/datasets/BLIP-3o/BLIP3o-Pretrain-Long-Caption, /data/datasets/BLIP-3o/BLIP3o-Pretrain-Short-Caption, /data/datasets/BLIP-3o/BLIP3o-Pretrain-JourneyDB] # [/data/datasets/BLIP-3o/BLIP3o-Pretrain-JourneyDB,]
      resolution: 512
      random_crop: false
      shuffle_buffer: 20000
      sample_shuffle: true
      repeat: true
  eval_dataset:
    class_path: src.data.dataset.geneval.GenEvalDataset
    init_args:
      meta_json_path: ./evaluations/geneval/evaluation_metadata.jsonl
      num_samples_per_instance: 4
      latent_shape:
        - 3
        - 512
        - 512
  pred_dataset:
    class_path: src.data.dataset.geneval.GenEvalDataset
    init_args:
      # meta_json_path: ./evaluations/geneval/evaluation_metadata_rephrased.jsonl
      meta_json_path: ./evaluations/geneval/evaluation_metadata.jsonl
      num_samples_per_instance: 4
      latent_shape:
        - 3
        - 512
        - 512
  train_batch_size: 16
  train_num_workers: 8
  pred_batch_size: 8
  pred_num_workers: 1